<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="Section-Linear-Transformations-of-Abstract-Vector-Spaces" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Linear Transformations of Abstract Vector Spaces</title>



 


<p>
Recall that a transformation <m>T:\mathbb{R}^n\rightarrow \mathbb{R}^m</m> is called a <term>linear transformation</term> if the following are true for all vectors <m>u</m> and <m>v</m> in <m>\mathbb{R}^n</m>, and scalars <m>k</m>.
<me>
T(ku)= kT(u),
</me>
<me>
T(u+v)= T(u)+T(v).
</me>

We generalize this definition as follows.
</p>




<definition xml:id="def-lintransgeneral">

    <statement>
        <p>
            Let <m>V</m> and <m>W</m> be vector spaces. A transformation <m>T:V\rightarrow W</m> is called a <term>linear transformation</term> if the following are true for all vectors <m>u</m> and <m>v</m> in <m>V</m>, and scalars <m>k</m>.
<me>
 T(k u)  = kT(u),
</me>
<me>
 T(u+v) = T(u)+T(v).
</me>
        </p>
    </statement>
</definition>

<p> 
This generalization allows for more interesting examples to be studied. For example:
</p> 

<example xml:id="ex-abstvectsplintransM22">
    <statement>
        <p>
            Recall that <m>\mathbb{M}_{n,n}</m> is the set of all <m>n\times n</m> matrices.  In <xref ref="ex-setofmatricesvectorspace"/>, we demonstrated that <m>\mathbb{M}_{n,n}</m> together with operations of matrix addition and scalar multiplication is a vector space.

Let <m>T_Q:\mathbb{M}_{n,n}\rightarrow \mathbb{M}_{n,n}</m> be a transformation defined by 
<me>
T_Q(A)=QA,
</me>
where <m>Q</m> is fixed <m>n\times n</m> matrix.  Show that <m>T_Q</m> is a linear transformation.
       </p>
    </statement>
    <answer>
        <p>
            We verify the linearity properties using properties of matrix-matrix and matrix-scalar multiplication (see <xref ref="th-propertiesofmatrixmultiplication"/>).  For <m>A</m> and <m>B</m> in <m>\mathbb{M}_{n,n}</m> and a scalar <m>k</m> we have:

<me>
    T_Q(kA)=Q(kA)=k(QA)=kT_Q(A)
</me>

together with

<me>
    T_Q(A+B)=Q(A+B)=QA+QB=T_Q(A)+T_Q(B).
</me>
       </p>
    </answer>
</example>

<example xml:id="ex-abstvecsplintrans2">
    <statement>
        <p>
            Recall that <m>\mathbb{P}^3</m> is the set of polynomials of degree <m>3</m> or less than <m>3</m>.  In <xref ref="ex-pnisavectorspace"/>, we showed that <m>\mathbb{P}^3</m> together with operations of polynomial addition and scalar multiplication is a vector space. 

Suppose <m>T:\R^3\rightarrow\mathbb{P}^3</m> is a linear transformation such that 

<me>
    T(\mathbf{i})=1+x-2x^2+x^3,
</me>


<me>
    T(\mathbf{j})=x+2x^3,
</me>


<me>
    T(\mathbf{k})=3+x^3.
</me>

Find the image of <m>\begin{bmatrix}1\\-2\\1\end{bmatrix}</m> under <m>T</m>.
       </p>
    </statement>
    <answer>
        <p>
            <md>
    <mrow> T\left(\begin{bmatrix}1\\-2\\1\end{bmatrix}\right)\amp =T(\mathbf{i}-2\mathbf{j}+\mathbf{k})=T(\mathbf{i})-2T(\mathbf{j})+T(\mathbf{k}) </mrow> 
    <mrow> \amp =(1+x-2x^2+x^3)-2(x+2x^3)+(3+x^3) </mrow>
    <mrow> \amp =4-x-2x^2-2x^3. </mrow>
</md>
       </p>
    </answer>
</example>

<example xml:id="ex-nonlinabstvectsp">
    <statement>
        <p>
            Let <m>T:\mathbb{M}_{3,3}\rightarrow \R</m> be a transformation such that <m>T(A)=\mbox{rank}(A)</m>.  Show that <m>T</m> is not linear.
       </p>
    </statement>
    <answer>
        <p>
            To show that <m>T</m> is not linear it suffices to find two matrices <m>A</m> and <m>B</m> such that <m>T(A+B)\neq T(A)+T(B)</m>.
        </p>
        
        <p>
        Observe that if we pick <m>A</m> and <m>B</m> so that each has rank <m>3</m> we would have <m>T(A)+T(B)=\mbox{rank}(A)+\mbox{rank}(B)=6</m> while <m>T(A+B)=\mbox{rank}(A+B)\leq 3</m>.  Clearly  <m>T(A+B)\neq T(A)+T(B)</m>. 
        </p> 

        <p>
        This argument is sufficient, but if we want to have a specific example, we can find one.
        </p>

        <p>
Let 
<me>
    A=\begin{bmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 1\end{bmatrix} \quad\text{and}\quad B=\begin{bmatrix}-1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp -1\end{bmatrix}.
</me>

Then

<me>
    T(A)=3\quad\text{and}\quad T(B)=3
</me>

and

<me>
    T(A+B)=T\left(\begin{bmatrix}0\amp 0\amp 0\\0\amp 2\amp 0\\0\amp 0\amp 0\end{bmatrix}\right)=1.
</me>

Thus, <m>1=T(A+B)\neq T(A)+T(B)=6</m>.
       </p>
    </answer>
</example>















<subsection xml:id="Subsection-Linear-Transformations-and-Bases">
    <title>Linear Transformations and Bases</title>


<exploration xml:id="init-tij">
    <p>
        Suppose we want to define a linear transformation <m>T:\R^2\rightarrow \R^2</m> by 
<me>
    T(\mathbf{i})=\begin{bmatrix}3\\-2\end{bmatrix}\quad\text{and}\quad T(\mathbf{j})=\begin{bmatrix}-1\\2\end{bmatrix}.
</me>
  
Is this information sufficient to define <m>T</m>?  
To answer this question we will try to determine what <m>T</m> does to an arbitrary vector of <m>\R^2</m>.  

If <m>\mathbf{v}</m> is a vector in <m>\R^2</m>, then <m>\mathbf{v}</m> can be uniquely expressed as a linear combination of <m>\mathbf{i}</m> and <m>\mathbf{j}</m>

<me>
    \mathbf{v}=a\mathbf{i}+b\mathbf{j}.
</me>
  By linearity of <m>T</m> we have 
<me>
    T(\mathbf{v})=T(a\mathbf{i}+b\mathbf{j})=aT(\mathbf{i})+bT(\mathbf{j})=a\begin{bmatrix}3\\-2\end{bmatrix}+b\begin{bmatrix}-1\\2\end{bmatrix}.
</me>

This shows that the image of every vector of <m>\R^2</m> under <m>T</m> is completely determined by the action of <m>T</m> on the standard unit vectors <m>\mathbf{i}</m> and <m>\mathbf{j}</m>.  

Vectors <m>\mathbf{i}</m> and <m>\mathbf{j}</m> form a standard basis of <m>\R^2</m>.  What if we want to use a different basis?  

Let 
<me>
\mathcal{B}=\left \lbrace \begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}2\\-1\end{bmatrix}\right \rbrace
</me>

be our basis of choice for <m>\R^2</m>. (How would you verify that <m>\mathcal{B}</m> is a basis of <m>\R^2</m>?) And suppose we want to define a linear transformation <m>S:\R^2\rightarrow \R^2</m> by 
<me>
    S\left(\begin{bmatrix}1\\1\end{bmatrix}\right)=\begin{bmatrix}0\\-1\end{bmatrix}\quad\text{and}\quad S\left(\begin{bmatrix}2\\-1\end{bmatrix}\right)=\begin{bmatrix}2\\0\end{bmatrix}.
</me>

Is this enough information to define <m>S</m>?

Because <m>[1,1],[2,-1]</m> form a basis of <m>\R^2</m>, every element <m>\mathbf{v}</m> of <m>\R^2</m> can be written as a unique linear combination 
<me>
    \mathbf{v}=a\begin{bmatrix}1\\1\end{bmatrix}+b\begin{bmatrix}2\\-1\end{bmatrix}.
</me>

We can find <m>S(\mathbf{v})</m> as follows:

<me>
    S(\mathbf{v})=S\left(a\begin{bmatrix}1\\1\end{bmatrix}+b\begin{bmatrix}2\\-1\end{bmatrix}\right)=a\begin{bmatrix}0\\-1\end{bmatrix}+b\begin{bmatrix}2\\0\end{bmatrix}.
</me>


Again, we see how a linear transformation is completely determined by its action on a basis.

<xref ref="th-uniquerep"/> assures us that given a basis, every vector has a unique representation as a linear combination of the basis vectors.  Imagine what would happen if this were not the case. 
</p>

    <p>
In the first part of this exploration, for instance, we might have been able to represent <m>\mathbf{v}</m> as <m>a\mathbf{i}+b\mathbf{j}</m> and <m>c\mathbf{i}+d\mathbf{j}</m> (<m>a\neq c</m> or <m>b\neq d</m>).  This would have resulted in <m>\mathbf{v}</m> mapping to two different elements: <m>aT(\mathbf{i})+bT(\mathbf{j})</m> and <m>cT(\mathbf{i})+dT(\mathbf{j})</m>, implying that <m>T</m> is not even a function.
    </p>
</exploration>


<p>
Let <m>\mathcal{B}=\{\mathbf{v}_1,\ldots,\mathbf{v}_p\}</m> be a basis of a vector space <m>V</m>.  To define a linear transformation <m>T:V\rightarrow W</m>, it is sufficient to state the image of each basis vector under <m>T</m>.  Once the images of the basis vectors are established, we can determine the images of all vectors of <m>V</m> as follows:

Given any vector <m>\mathbf{v}</m> of <m>V</m>, write <m>\mathbf{v}</m> as a linear combination of the elements of <m>\mathcal{B}</m>

<me>
    \mathbf{v}=a_1\mathbf{v}_1+\ldots+a_p\mathbf{v}_p.
</me>
 
Then

<me>
    T(\mathbf{v})=T(a_1\mathbf{v}_1+\ldots+a_p\mathbf{v}_p)=a_1T(\mathbf{v}_1)+\ldots+a_pT(\mathbf{v}_p).
</me>
</p>
</subsection>



















<subsection xml:id="Subsection-Coordinate-Vectors">
    <title>Coordinate Vectors</title>

    <p>
Transformations that map vectors to their coordinate vectors will prove to be of great importance.  In this section we will prove that such transformations are linear and give several examples.

If <m>V</m> is a vector space, and  <m>\mathcal{B}=\{\mathbf{v}_1, \ldots ,\mathbf{v}_n\}</m> is an ordered basis for <m>V</m> then any vector <m>\mathbf{v}</m> of <m>V</m> can be uniquely expressed as <m>\mathbf{v}=a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n</m> for some scalars <m>a_1, \ldots ,a_n</m>.  Vector <m>[\mathbf{v}]_{\mathcal{B}}</m> in <m>\R^n</m> given by 

<me>
    [\mathbf{v}]_{\mathcal{B}}=\begin{bmatrix}a_1\\a_2\\\vdots\\a_n\end{bmatrix}
</me>

is said to be the <term>coordinate vector</term> for <m>\mathbf{v}</m> with respect to the ordered basis <m>\mathcal{B}</m> (see <xref ref="def-coordvector"/>).  

It turns out that the transformation <m>T:V\rightarrow \R^n</m> defined by 
<me>
T(\mathbf{v})=[\mathbf{v}]_{\mathcal{B}}
</me>
is linear.  Before we prove linearity of <m>T</m>, consider the following examples.
</p>


<example xml:id="ex-abstvectsplintranscoordvect1">
    <statement>
        <p>
            Consider <m>\mathbb{M}_{2,2}</m>.  Let 
            <me>
            \mathcal{B}=\left\{\begin{bmatrix}1\amp 0\\0\amp 0\end{bmatrix}, \begin{bmatrix}0\amp 1\\0\amp 0\end{bmatrix}, \begin{bmatrix}0\amp 0\\1\amp 0\end{bmatrix}, \begin{bmatrix}0\amp 0\\0\amp 1\end{bmatrix}\right\}
            </me>
            
            be an ordered basis for <m>\mathbb{M}_{2,2}</m>  (You should do a quick mental check that <m>\mathcal{B}</m> is a legitimate basis).  Define <m>T:\mathbb{M}_{2,2}\rightarrow \R^4</m> by <m>T(A)=[A]_{\mathcal{B}}</m>.  Find 
            <me>
            T\left(\begin{bmatrix}-2\amp 3\\1\amp -5\end{bmatrix}\right).
            </me>
       </p>
    </statement>
    <answer>
        <p>
            We need to find the coordinate vector for <m>\begin{bmatrix}-2\amp 3\\1\amp -5\end{bmatrix}</m> with respect to <m>\mathcal{B}</m>. Firstly,

<me>
    \begin{bmatrix}-2\amp 3\\1\amp -5\end{bmatrix}=-2\begin{bmatrix}1\amp 0\\0\amp 0\end{bmatrix}+ 3\begin{bmatrix}0\amp 1\\0\amp 0\end{bmatrix}+ \begin{bmatrix}0\amp 0\\1\amp 0\end{bmatrix}+ (-5)\begin{bmatrix}0\amp 0\\0\amp 1\end{bmatrix}.
</me>

This gives us:

<me>
    T\left(\begin{bmatrix}-2\amp 3\\1\amp -5\end{bmatrix}\right)=\left[\begin{bmatrix}-2\amp 3\\1\amp -5\end{bmatrix}\right]_{\mathcal{B}}=\begin{bmatrix}-2\\3\\1\\-5\end{bmatrix}.
</me>
       </p>
    </answer>
</example>

<example xml:id="ex-abstvectsplintranspoly">
    <statement>
        <p>
            Recall that <m>\mathbb{P}^2</m> is the set of polynomials of degree <m>2</m> or less than <m>2</m>.  In <xref ref="ex-deg-le-2vectorspace"/>, we showed that <m>\mathbb{P}^2</m> is a vector space. 
<ol>
    <li xml:id="item-lintranspolycoordvect1">
  <p>  Let <m>\mathcal{B}_1=\{1, x, x^{2}\}</m> be an ordered basis for <m>\mathbb{P}^2</m>.  (It is easy to verify that <m>\mathcal{B}_1</m> is a basis.) If <m>T:\mathbb{P}^2\rightarrow \R^3</m> is given by <m>T(p)=[p]_{\mathcal{B}_1}</m>, find 
    <me>
    T(2x^2-3x).
    </me> </p>
</li>
    <li xml:id="item-lintranspolycoordvect2">
  <p> 
Let <m>\mathcal{B}_2=\{1 + x, 1 - x, x + x^{2}\}</m> be an ordered basis for <m>\mathbb{P}^2</m>- In (see <xref ref="prob-linindabstractvsp1"/>), you demonstrated that <m>\mathcal{B}_2</m> is a basis.)  If <m>T:\mathbb{P}^2\rightarrow \R^3</m> is given by <m>T(p)=[p]_{\mathcal{B}_2}</m>, find
 <me>
T(2x^2-3x).
</me> </p>
</li>
</ol>
       </p>
    </statement>
    <answer>
        <p>
            <xref ref="item-lintranspolycoordvect1"/>  We express <m>2x^2-3x</m> as a linear combination of elements of <m>\mathcal{B}_1</m>.

<me>
    2x^2-3x=0\cdot 1+ (-3)x+2x^2.
</me>

Therefore 
<me>
    [2x^2-3x]_{\mathcal{B}_1}=\begin{bmatrix}0\\-3\\2\end{bmatrix}.
</me>

Note that it is important to keep the basis elements in the same order in which they are listed, as the order of components of the coordinate vector depends on the order of the basis elements.  We conclude that

<me>
    T(2x^2-3x)=\begin{bmatrix}0\\-3\\2\end{bmatrix}.
</me>


For <xref ref="item-lintranspolycoordvect2"/>: Our goal is to express <m>2x^2-3x</m> as a linear combination of the elements of <m>\mathcal{B}_2</m>.  Thus, we need to find coefficients <m>a</m>, <m>b</m> and <m>c</m> such that

<md>
<mrow>    2x^2-3x \amp =a(1+x)+b(1-x)+c(x+x^2) </mrow>
<mrow>    \amp =(a+b)+(a-b+c)x+cx^2. </mrow>
</md>

This gives us a system of linear equations:

<me>
    \begin{array}{ccccccc}
      a \amp  +\amp b\amp \amp \amp = \amp 0 \\
	 a\amp  -\amp b\amp +\amp c\amp =\amp -3\\
     \amp  \amp \amp \amp c\amp =\amp 2
    \end{array}
</me>

    Solving the system yields <m>a=-\frac{5}{2}</m>, <m>b=\frac{5}{2}</m> and <m>c=2</m>.  Thus
    
<me>
    T(2x^2-3x)=[2x^2-3x]_{\mathcal{B}_2}=\begin{bmatrix}-5/2\\5/2\\2\end{bmatrix}.
</me>
       </p>
    </answer>
</example>

<theorem xml:id="th-coordvectmappinglinear">

    <statement>
        <p>
            Let <m>V</m> be an <m>n</m>-dimensional vector space, and let <m>\mathcal{B}</m> be an ordered basis for <m>V</m>.  Then  <m>T:V\rightarrow \R^n</m> given by <m>T(\mathbf{v})=[\mathbf{v}]_{\mathcal{B}}</m> is a linear transformation.
        </p>
    </statement>
</theorem>
<proof>
    <p>
First observe that <xref ref="th-uniquerep"/> of guarantees that there is only one way to represent each element of <m>V</m> as a linear combination of elements of <m>\mathcal{B}</m>.  Thus each element of <m>V</m> maps to exactly one element of <m>\R^n</m>, as long as the order in which elements of <m>\mathcal{B}</m> appear is taken into account.  This proves that <m>T</m> is a function, or a transformation.  
    </p>

    <p>
We will now prove that <m>T</m> is linear.

Let <m>\mathbf{v}</m> be an element of <m>V</m>.  We will first show that <m>T(k\mathbf{v})=kT(\mathbf{v})</m>.  Suppose <m>\mathcal{B}=\{\mathbf{v}_1, \ldots ,\mathbf{v}_n\}</m>, then <m>\mathbf{v}</m> can be written as a unique linear combination:

<me>
    \mathbf{v}=a_1\mathbf{v}_1+ \ldots +a_n\mathbf{v}_n
</me>

We have:
<md>
<mrow>    T(k\mathbf{v})\amp =T(k(a_1\mathbf{v}_1+ \ldots +a_n\mathbf{v}_n)) </mrow>
<mrow>    \amp =T((ka_1)\mathbf{v}_1+ \ldots +(ka_n)\mathbf{v}_n)  </mrow>
<mrow>    \amp =\begin{bmatrix}ka_1\\\vdots\\ka_n\end{bmatrix}=k\begin{bmatrix}a_1\\\vdots\\a_n\end{bmatrix}=kT(\mathbf{v}). </mrow>
</md>
We leave it to the reader to verify that <m>T(\mathbf{v}+\mathbf{w})=T(\mathbf{v})+T(\mathbf{w})</m> (see <xref ref="prob-completeproofoflin"/>).
    </p>
</proof>

<p>
In our final example, we will consider <m>T</m> in the context of a basis of the codomain, as well as a basis of the domain.  
This will later help us tackle the question of the matrix of <m>T</m> associated with bases other than the standard one.
</p> 


<example xml:id="ex-subtosub1">
    <statement>
        <p>
            Let

<me>
    \mathbf{v}_1=\begin{bmatrix}1\\2\\0\end{bmatrix}\quad\text{and}\quad\mathbf{v}_2=\begin{bmatrix}0\\1\\1\end{bmatrix},
</me>


<me>
    \mathbf{w}_1=\begin{bmatrix}1\\0\\1\end{bmatrix}\quad\text{and}\quad\mathbf{w}_2=\begin{bmatrix}1\\0\\0\end{bmatrix},
</me>

and
<me>
    V=\text{span}(\mathbf{v}_1, \mathbf{v}_2)\quad\text{and}\quad W=\text{span}(\mathbf{w}_1, \mathbf{w}_2).
</me>


Because each of <m>\{\mathbf{v}_1, \mathbf{v}_2\}</m> and <m>\{\mathbf{w}_1, \mathbf{w}_2\}</m> is linearly independent, let 

<me>
    \mathcal{B}=\{\mathbf{v}_1, \mathbf{v}_2\}\quad\text{and}\quad\mathcal{C}=\{\mathbf{w}_1, \mathbf{w}_2\}
</me>

be ordered bases of <m>V</m> and <m>W</m>, respectively.


Define a linear transformation <m>T:V\rightarrow W</m> by 

<me>
    T(\mathbf{v}_1)=2\mathbf{w}_1-3\mathbf{w}_2\quad\text{and} \quad T(\mathbf{v}_2)=-\mathbf{w}_1+4\mathbf{w}_2.
</me>


<ol>
<li xml:id="item-subtosub1a">
  <p> 
Verify that <m>\mathbf{v}=[2,5,1]</m> is in <m>V</m> and find the coordinate vector <m>[\mathbf{v}]_{\mathcal{B}}</m>. </p>
</li>
<li xml:id="item-subtosub1b">
  <p> 
Find <m>T(\mathbf{v})</m> and the coordinate vector <m>[T(\mathbf{v})]_{\mathcal{C}}</m>. </p>
</li>
</ol>
       </p>
    </statement>
    <answer>
        <p>
            For <xref ref="item-subtosub1a"/>, we need to express <m>\mathbf{v}</m> as a linear combination of <m>\mathbf{v}_1</m> and <m>\mathbf{v}_2</m>.  This can be done by observation or by solving the equation

<me>
    \begin{bmatrix}1\amp 0\\2\amp 1\\0\amp 1\end{bmatrix}\begin{bmatrix}a\\b\end{bmatrix}=\begin{bmatrix}2\\5\\1\end{bmatrix}.
</me>

We find that <m>a=2</m> and <m>b=1</m>, so <m>\mathbf{v}=2\mathbf{v}_1+\mathbf{v}_2</m>.  Thus <m>\mathbf{v}</m> is in <m>V</m>.  The coordinate vector for <m>\mathbf{v}</m> with respect to the ordered basis <m>\mathcal{B}</m> is 

<me>
    [\mathbf{v}]_{\mathcal{B}}=\begin{bmatrix}2\\1\end{bmatrix}.
</me>


For <xref ref="item-subtosub1b"/>, by linearity of <m>T</m> we have 
<md>
<mrow> T(\mathbf{v})=T(2\mathbf{v}_1+\mathbf{v}_2)\amp =2T(\mathbf{v}_1)+T(\mathbf{v}_2) </mrow>
<mrow> \amp =2(2\mathbf{w}_1-3\mathbf{w}_2)+(-\mathbf{w}_1+4\mathbf{w}_2) </mrow>
<mrow> \amp =3\mathbf{w}_1-2\mathbf{w}_2=\begin{bmatrix}1\\0\\3\end{bmatrix}. </mrow>
</md>

The coordinate vector for <m>T(\mathbf{v})</m> with respect to the ordered basis <m>\mathcal{C}</m> is 

<me>
    [T(\mathbf{v})]_{\mathcal{C}}=\begin{bmatrix}3\\-2\end{bmatrix}.
</me>
       </p>
    </answer>
</example>
</subsection>



























<subsection xml:id="Section-Existence-of-the-Inverse-of-a-Linear-Transformation">
    <title>Inverses of a Linear Transformations</title>



  



 <p>
In <xref ref="ep-inverse"/>, we examined a linear transformation <m>T:\R^2\rightarrow \R^2</m> that doubles all input vectors, 
and its inverse <m>S:\R^2\rightarrow \R^2</m>, that halves all input vectors.  
We observed that the composite functions <m>S\circ T</m> and <m>T\circ S</m> are both identity transformations.  
Diagrammatically, we can represent <m>T</m> and <m>S</m> as follows:
 </p> 

 
<image width="65%">
   <shortdescription>Idea of inverse diagram</shortdescription>
    <latex-image>
      \begin{tikzpicture}
 \node[] at (0, -1.2)  (top)    {\(T\) doubles each input};
 \node[] at (-3, -2.5)  (left1)    {\(\mathbf{v}\)};
 \node[] at (3, -2.5)  (right1)    {\(2\mathbf{v}\)};
 \node[] at (0, -3.7)  (bottom)    {\(S\) halves each input};
  \draw [-\gt ,line width=0.5pt,-stealth]  (left1.north east)to[out=30, in=150](right1.north west);
 \draw [-\gt ,line width=0.5pt,-stealth]  (right1.south west)to[out=210, in=330](left1.east);
     \end{tikzpicture}
    </latex-image>
</image>
 
<p>
This gives us a way of thinking about an inverse of <m>T</m> as a transformation that ``undoes" the action of <m>T</m> by ``reversing" the mapping arrows.  We will now use these intuitive ideas to understand which linear transformations are invertible and which are not.
</p> 

<p> 
Given an arbitrary linear transformation <m>T:V\rightarrow W</m>, ``reversing the arrows"
 may not always result in a transformation. Recall that transformations are functions.  
 The figures below show two ways in which our attempt to ``reverse" <m>T</m> may fail to produce a function.
 
 First, if two distinct vectors <m>\mathbf{v}_1</m> and <m>\mathbf{v}_2</m> map to the same vector <m>\mathbf{w}</m> in <m>W</m>, 
 then reversing the arrows gives us a mapping that is clearly not a function. 
</p>


<image width="75%">
   <shortdescription></shortdescription>
    <latex-image>
      \begin{tikzpicture}
 \node[] at (-5, 1.2)  (topleft)    {\(\mathbf{v}_1\)};
 \node[] at (-5, -1.2)  (bottomleft)    {\(\mathbf{v}_2\)};
 \node[] at (0, 0)  (cleft1)    {\(T(\mathbf{v}_1)=T(\mathbf{v}_2)=\mathbf{w}\)};
 
 \node[gray] at (-5, 0)  (comment)    {(Two distinct elements map to \(\mathbf{w}\))};
   \draw [-\gt ,line width=0.5pt,-stealth]  (topleft.east) to (cleft1.north west);
  \draw [-\gt ,line width=0.5pt,-stealth]  (bottomleft.east) to (cleft1.south west);
     \end{tikzpicture}
    </latex-image>
</image>

<image width="75%">
   <shortdescription></shortdescription>
    <latex-image>
      \begin{tikzpicture}
 \node[] at (-5, 1.2)  (topleft)    {\(\mathbf{v}_1\)};
 \node[] at (-5, -1.2)  (bottomleft)    {\(\mathbf{v}_2\)};
 \node[] at (0, 0)  (cleft1)    {\(T(\mathbf{v}_1)=T(\mathbf{v}_2)=\mathbf{w}\)};
 \node[gray] at (-6, 0)  (comment)    {(Reversing the arrows does not produce a function)};
   \draw [-\gt ,line width=0.5pt,-stealth]  (cleft1.north west) to (topleft.east);
  \draw [-\gt ,line width=0.5pt,-stealth]  (cleft1.south west) to (bottomleft.east);
  \node[] at (-4, -2)  (caption)    {Figure 1.  Two distinct elements map to \(\mathbf{w}\).};
     \end{tikzpicture}
    </latex-image>
</image>

  
<p>
Second, observe that our definition of an inverse of <m>T:V\rightarrow W</m> requires that the domain of the inverse transformation be
 <m>W</m> (remember the inverse is intuitively the opposite one!)).  If there is a vector <m>\mathbf{b}</m> in <m>W</m> that is not an image of any vector in <m>V</m>, 
 then <m>\mathbf{b}</m> cannot be in the domain of an inverse transformation. 
</p> 


<image width="75%">
   <shortdescription></shortdescription>
    <latex-image>
      \begin{tikzpicture}
 \node[] at (0, 3)  (topleft)    {\(\mathbf{b}\)};
 \node[] at (-2, 2)  (bottomleft)    {\(\mathbf{v}\)};
 \node[] at (0, 2)  (cleft1)    {\(T(\mathbf{v})\)};
  \node[gray] at (2, 3)  (comment)    {(Nothing maps to \(\mathbf{b}\))};
  \draw [-\gt ,line width=0.5pt,-stealth]  (bottomleft.east) to (cleft1.west);
  \node[] at (0, 1)  (topright)    {\(\mathbf{b}\)};
 \node[] at (-2, 0)  (bottomleft)    {\(\mathbf{v}\)};
 \node[] at (-2, 1)  (topleft)    {?};
 \node[] at (0, 0)  (cleft1)    {\(T(\mathbf{v})\)};
 \node[gray] at (3, 1)  (comment)    {(\(\mathbf{b}\) has nothing to map ``back" to)};
  
  \draw [-\gt ,line width=0.5pt,-stealth]  (cleft1.west) to (bottomleft.east);
  \draw [-\gt ,line width=0.5pt,-stealth]  (topright.west) to (topleft.east);
  \node[] at (0, -1)  (caption)    {Figure 2.  What happens when nothing maps to \(\mathbf{b}\).};
     \end{tikzpicture}
    </latex-image>
</image>
 

<p> 
We now illustrate these potential issues with specific examples.
</p> 



<example xml:id="ex-notonetoone">
    <statement>
        <p>
            Let <m>T:\R^2\rightarrow \R^2</m> be a linear transformation whose standard matrix is

<me>
    \begin{bmatrix}1\amp 1\\2\amp 2\end{bmatrix}.
</me>

Does <m>T</m> have an inverse? Show that multiple vectors of the domain map to <m>\mathbf{0}</m> in the codomain.
       </p>
    </statement>

    <answer>
        <p>
            The matrix 
        <me>
        \begin{bmatrix}1\amp 1\\2\amp 2\end{bmatrix}
        </me>
        
        is not invertible, so <m>T</m> does not have an inverse.

We now dig a little deeper to get additional insights into why <m>T</m> does not have an inverse.  Observe that all vectors of the form <m>[k,-k]</m> map to <m>\mathbf{0}</m>.  To verify this, use matrix multiplication:

<me>
    \begin{bmatrix}1\amp 1\\2\amp 2\end{bmatrix}\begin{bmatrix}k\\-k\end{bmatrix}=\begin{bmatrix}0\\0\end{bmatrix}.
</me>

This shows that there are infinitely many vectors that map to <m>\mathbf{0}</m>.  So, ``reversing the arrows" would not result in a function. (See Figure 1)
       </p>
    </answer>
</example>



<example xml:id="ex-notonto">
    <statement>
        <p>
            Let <m>T:\R^2\rightarrow \R^3</m> be a linear transformation whose standard matrix is

<me>
    \begin{bmatrix}1\amp 0\\0\amp 1\\2\amp 0\end{bmatrix}
</me>

Does <m>T</m> have an inverse? Show that there exists a vector <m>\mathbf{b}</m> in <m>\R^3</m> such that no vector of <m>\R^2</m> maps to <m>\mathbf{b}</m>.
       </p>
    </statement>
    <answer>
        <p>
            The matrix 
        <me>
        \begin{bmatrix}1\amp 0\\0\amp 1\\2\amp 0\end{bmatrix}
        </me>
        
        is not invertible (it's not even a square matrix!), so <m>T</m> does not have an inverse.

We now get another insight into why <m>T</m> is not invertible.
To find a vector <m>\mathbf{b}</m> such that no vector of <m>\R^2</m> maps to <m>\mathbf{b}</m>, we need to find <m>\mathbf{b}</m> for which the matrix equation
<men xml:id="ex-matrix">
\begin{bmatrix}1\amp 0\\0\amp 1\\2\amp 0\end{bmatrix}\mathbf{x}=\mathbf{b}.
</men>

has no solution.  
</p> 

<p> 
Let <m>[b_1, b_2, b_3]</m>.  Gauss-Jordan elimination yields:


<me>
    \left[\begin{array}{cc|c}  
 1 \amp  0 \amp  b_1\\  
 0 \amp  1 \amp  b_2\\
 2 \amp  0 \amp  b_3
\end{array}\right] \rightsquigarrow \left[\begin{array}{cc|c} 
 1 \amp  0 \amp  b_1\\  
 0 \amp  1 \amp  b_2\\
 0 \amp  0 \amp  b_3-2b_1
\end{array}\right]
</me>

Now, <xref ref="ex-matrix"/> has a solution if and only if <m>b_3-2b_1=0</m>.  Since we do not want <xref ref="ex-matrix"/> to have a solution, all we need to do is pick values <m>b_1</m>, <m>b_2</m> and <m>b_3</m> such that <m>b_3-2b_1\neq 0</m>.  Let <m>\mathbf{b}=[1,1,1]</m>.  Then no element of <m>\R^2</m> maps to <m>\mathbf{b}</m>.  This shows that we cannot ``reverse the arrows" in an attempt to produce an inverse of <m>T</m>. (See Figure 2)
       </p>
    </answer>
</example>

<p>
Our next goal is to develop vocabulary that would allow us to discuss issues illustrated in Figures <m>1</m> and <m>2</m>.
</p> 
</subsection>





















<subsection xml:id="Subsection-One-to-one-Linear-Transformations">
    <title>One-to-one and Onto Linear Transformations</title>

    <p>
Figure <m>1</m> gave us a diagrammatic representation of a transformation that maps two distinct elements, <m>\mathbf{v}_1</m> and <m>\mathbf{v}_2</m> to the same element <m>\mathbf{w}</m>, making it impossible for us to ``reverse the arrows" in an attempt to find the inverse transformation.  Based on this example, it is reasonable to conjecture that for a transformation to be invertible, the transformation must be such that each output is the image of exactly one input.  Such transformations are called <term>one-to-one</term>.  
    </p>

<definition xml:id="def-onetoone">
    <title>One-to-One</title>
    <statement>
        <p>
            A linear transformation <m>T:V\rightarrow W</m> is <term>one-to-one</term> if 

<me>
    T(\mathbf{v}_1)=T(\mathbf{v}_2)\quad \text{implies that}\quad \mathbf{v}_1=\mathbf{v}_2.
</me>
        </p>
    </statement>
</definition>

<p>
The transformation in figure <m>1</m> is not one-to-one because <m>\mathbf{v}_1</m> and <m>\mathbf{v}_2</m> map to the same vector <m>\mathbf{w}</m>, (i.e. <m>T(\mathbf{v}_1)=T(\mathbf{v}_2)</m>), yet the diagram suggests that <m>\mathbf{v}_1\neq\mathbf{v}_2</m>.
</p> 

<p>
Let us rexamine the previous examples with this new terminology.
</p> 


<example xml:id="ex-notonetoone2">
    <statement>
        <p>
            Transformation <m>T</m> in <xref ref="ex-notonetoone"/> is not one-to-one.
       </p>
    </statement>
    <answer>
        <p>
            We can use any two vectors of the form <m>\begin{bmatrix}k\\-k\end{bmatrix}</m> to make our case.  

<me>
    T\left(\begin{bmatrix}1\\-1\end{bmatrix}\right)=\mathbf{0}=T\left(\begin{bmatrix}-2\\2\end{bmatrix}\right)\quad \text{but}\quad\begin{bmatrix}1\\-1\end{bmatrix}\neq \begin{bmatrix}-2\\2\end{bmatrix}.
</me>

In other words, we have more than one vector that maps to the zero vector.
       </p>
    </answer>
</example>

<example xml:id="ex-notontoisonetoone">
    <p>
        Prove that the transformation in <xref ref="ex-notonto"/> is one-to-one.
<answer>
    <p>
Suppose

<me>
    T\left(\begin{bmatrix}x_1\\x_2\end{bmatrix}\right)=T\left(\begin{bmatrix}y_1\\y_2\end{bmatrix}\right)
</me>

Then

<me>
    \begin{bmatrix}1\amp 0\\0\amp 1\\2\amp 0\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix}=\begin{bmatrix}1\amp 0\\0\amp 1\\2\amp 0\end{bmatrix}\begin{bmatrix}y_1\\y_2\end{bmatrix}.
</me>


<me>
    x_1\begin{bmatrix}1\\0\\2\end{bmatrix}+x_2\begin{bmatrix}0\\1\\0\end{bmatrix}=y_1\begin{bmatrix}1\\0\\2\end{bmatrix}+y_2\begin{bmatrix}0\\1\\0\end{bmatrix}.
</me>


<me>
    (x_1-y_1)\begin{bmatrix}1\\0\\2\end{bmatrix}+(x_2-y_2)\begin{bmatrix}0\\1\\0\end{bmatrix}=\mathbf{0}.
</me>

It is clear that <m>\begin{bmatrix}1\\0\\2\end{bmatrix}</m> and <m>\begin{bmatrix}0\\1\\0\end{bmatrix}</m> are linearly independent.  Therefore, we must have <m>x_1-y_1=0</m> and <m>x_2-y_2=0</m>.  But then <m>x_1=y_1</m> and <m>x_2=y_2</m>, so 

<me>
    \begin{bmatrix}x_1\\x_2\end{bmatrix}=\begin{bmatrix}y_1\\y_2\end{bmatrix}.
</me>

    </p>
</answer>
    </p>
</example>

<p>
Since transformation in <xref ref="ex-notonto"/> is one-to-one but not invertible we can conjecture that being one-to-one is a necessary, but not a sufficient condition for a linear transformation to have an inverse.  We will consider the other necessary condition next.
</p> 


<p>
Figure <m>2</m> makes a convincing case that for a transformation to be invertible every element of the codomain must have something mapping to it. Transformations such that every element of the codomain is an image of some element of the domain are called <term>onto</term>.
</p>

<definition xml:id="def-onto">
    <title>Onto</title>
    <statement>
        <p>
            A linear transformation <m>T:V\rightarrow W</m> is <term>onto</term> if for every element <m>\mathbf{w}</m> of <m>W</m>, there exists an element <m>\mathbf{v}</m> of <m>V</m> such that <m>T(\mathbf{v})=\mathbf{w}</m>.
        </p>
    </statement>
</definition>


<p> 
Once again, we place preceding examples in the light of "onto".
</p> 

<example xml:id="ex-notonto2">
    <statement>
        <p>
            The transformation in <xref ref="ex-notonto"/> is not onto.
       </p>
    </statement>
    <answer>
        <p>
            No element of <m>\R^2</m> maps to <m>\begin{bmatrix}1\\1\\1\end{bmatrix}</m>.
       </p>
    </answer>
</example>

<example xml:id="ex-onto1">
    <statement>
        <p>
            Prove that the linear transformation <m>T:\R^2\rightarrow \R^2</m> whose standard matrix is 
<me>
    A=\begin{bmatrix}1\amp 0\\2\amp 1\end{bmatrix}
</me>
 is onto.
       </p>
    </statement>
    <answer>
        <p>
            Let <m>\mathbf{b}</m> be an element of the codomain (<m>\R^2</m>).  We need to find <m>\mathbf{x}</m> in the domain (<m>\R^2</m>) such that <m>T(\mathbf{x})=\mathbf{b}</m>. 
Observe that <m>A</m> is invertible, and 
<me>
    A^{-1}=\begin{bmatrix}1\amp 0\\-2\amp 1\end{bmatrix}.
</me>

 Let <m>\mathbf{x}=\begin{bmatrix}1\amp 0\\-2\amp 1\end{bmatrix}\mathbf{b}</m>, then 
 
<me>
    T(\mathbf{x})=\begin{bmatrix}1\amp 0\\2\amp 1\end{bmatrix}\left(\begin{bmatrix}1\amp 0\\-2\amp 1\end{bmatrix}\mathbf{b}\right)=I\mathbf{b}=\mathbf{b}.
</me>
       </p>
    </answer>
</example>

<example xml:id="ex-onto2">
    <statement>
        <p>
            Prove that the linear transformation <m>T:\R^3\rightarrow \R^2</m> induced by 
<me>
    A=\begin{bmatrix}1\amp 1\amp -1\\2\amp 3\amp -1\end{bmatrix}
</me>
 is onto.
       </p>
    </statement>
    <answer>
        <p>
            Let <m>\mathbf{b}</m> be an element of <m>\R^2</m>.  We need to show that there exists <m>\mathbf{x}</m> in <m>\R^3</m> such that <m>T(\mathbf{x})=A\mathbf{x}=\mathbf{b}</m>. 
Observe that
 
<me>
    \mbox{rref}(A)=\begin{bmatrix}1 \amp  0 \amp  -2\\0 \amp  1 \amp  1\end{bmatrix}.
</me>

 This means that <m>A\mathbf{x}=\mathbf{b}</m> has a solution (in fact, it has infinitely many solutions) for every <m>\mathbf{b}</m> in <m>\R^2</m>.  Therefore every <m>\mathbf{b}</m> in <m>\R^2</m> is an image of some <m>\mathbf{x}</m> in <m>\R^3</m>. We conclude that <m>T</m> is onto.
       </p>
    </answer>
</example>

<example xml:id="ex-subtosub">
    <statement>
        <p>
            Let 
<me>
    V=\text{span}\left(\begin{bmatrix}1\\0\\0\end{bmatrix}, \begin{bmatrix}1\\1\\1\end{bmatrix}\right).
</me>

Define a linear transformation 
<me>
    T:V\rightarrow \R^2
</me>

by 
<me>
    T\left(\begin{bmatrix}1\\0\\0\end{bmatrix}\right)=\begin{bmatrix}1\\1\end{bmatrix}\quad \text{and} \quad T\left(\begin{bmatrix}1\\1\\1\end{bmatrix}\right)=\begin{bmatrix}0\\1\end{bmatrix}.
</me>

Show that <m>T</m> is one-to-one and onto.
       </p>
    </statement>
    <answer>
        <p>
            We will now show that <m>T</m> is one-to-one.  
Suppose 

<me>
    T(\mathbf{u})=T(\mathbf{v})
</me>

for some <m>\mathbf{u}</m> and <m>\mathbf{v}</m> in <m>V</m>. Vectors <m>\mathbf{u}</m> and <m>\mathbf{v}</m> are in the span of <m>\begin{bmatrix}1\\0\\0\end{bmatrix}</m> and <m>\begin{bmatrix}1\\1\\1\end{bmatrix}</m>, so


<me>
    \mathbf{u}=a\begin{bmatrix}1\\0\\0\end{bmatrix}+b\begin{bmatrix}1\\1\\1\end{bmatrix}\quad\text{and}\quad \mathbf{v}=c\begin{bmatrix}1\\0\\0\end{bmatrix}+d\begin{bmatrix}1\\1\\1\end{bmatrix}
</me>

for some scalars <m>a, b, c, d</m>.


<md>
<mrow>    T(\mathbf{u}) \amp =aT\left(\begin{bmatrix}1\\0\\0\end{bmatrix}\right)+bT\left(\begin{bmatrix}1\\1\\1\end{bmatrix}\right)  =a\begin{bmatrix}1\\1\end{bmatrix}+b\begin{bmatrix}0\\1\end{bmatrix}  =\begin{bmatrix}a\\a+b\end{bmatrix}, </mrow> 
<mrow>    T(\mathbf{v}) \amp =cT\left(\begin{bmatrix}1\\0\\0\end{bmatrix}\right)+dT\left(\begin{bmatrix}1\\1\\1\end{bmatrix}\right)  =c\begin{bmatrix}1\\1\end{bmatrix}+d\begin{bmatrix}0\\1\end{bmatrix}  =\begin{bmatrix}c\\c+d\end{bmatrix}. </mrow>
</md>

Thus,

<me>
    \begin{bmatrix}a\\a+b\end{bmatrix}=\begin{bmatrix}c\\c+d\end{bmatrix}.
</me>

This implies that <m>a=c</m> which, in turn, implies <m>b=d</m>.  This gives us <m>\mathbf{u}=\mathbf{v}</m>, and we conclude that <m>T</m> is one-to-one.
</p>

<p> 
Next we will show that <m>T</m> is onto.  
The key observation is that vectors <m>[1,1]</m> and <m>[0,1]</m> span <m>\R^2</m>. This means that given a vector <m>\mathbf{v}</m> in <m>\R^2</m>, we can write <m>\mathbf{v}</m> as 
<me>
\mathbf{v}=a\begin{bmatrix}1\\1\end{bmatrix}+b\begin{bmatrix}0\\1\end{bmatrix}.
</me> 

But this means that 
<m>
\mathbf{v}=T\left(a\begin{bmatrix}1\\0\\0\end{bmatrix}+b\begin{bmatrix}1\\1\\1\end{bmatrix}\right).
</m> 

We conclude that <m>T</m> is onto.
       </p>
    </answer>
</example>
</subsection>
















<subsection xml:id="Subsection-Existence-of-Inverses">
    <title>Existence and Uniqueness of Inverses</title>

<theorem xml:id="th-isomeansinvert">

    <statement>
        <p>
            Let <m>V</m> and <m>W</m> be vector spaces, and let <m>T:V\rightarrow W</m> be a linear transformation.  Then <m>T</m> has an inverse if and only if <m>T</m> is one-to-one and onto.
        </p>
    </statement>

<proof>
    <p>
We will first assume that <m>T</m> is one-to-one and onto, and show that there exists a transformation <m>S:W\rightarrow V</m> such that <m>S\circ T=\id_V</m> and <m>T\circ S=\id_W</m>. 
    </p>
    
<p>
Because <m>T</m> is onto, for every <m>\mathbf{w}</m> in <m>W</m>, there exists <m>\mathbf{v}</m> in <m>V</m> such that <m>T(\mathbf{v})=\mathbf{w}</m>.  Moreover, because <m>T</m> is one-to-one, vector <m>\mathbf{v}</m> is the only vector that maps to <m>\mathbf{w}</m>.  To stress this, we will say that for every <m>\mathbf{w}</m>, there exists <m>\mathbf{v}_{\mathbf{w}}</m> such that <m>T(\mathbf{v}_{\mathbf{w}})=\mathbf{w}</m>. (Since every <m>\mathbf{v}</m> maps to exactly one <m>\mathbf{w}</m>, this notation makes sense for elements of <m>V</m> as well.)  We can now define <m>S:W\rightarrow V</m> by <m>S(\mathbf{w})=\mathbf{v}_{\mathbf{w}}</m>.
Then

<md>
<mrow>    (S\circ T)(\mathbf{v}_{\mathbf{w}})  \amp=  S(T(\mathbf{v}_{\mathbf{w}})) \amp =S (\mathbf{w}) = \mathbf{v}_{\mathbf{w}}, </mrow>
<mrow>    (T\circ S)(\mathbf{w})               \amp=  T(S(\mathbf{w}))              \amp =T (\mathbf{v}_{\mathbf{w}}) = \mathbf{w}. </mrow>
</md>

We conclude that <m>S\circ T=\id_V</m> and <m>T\circ S=\id_W</m>.  Therefore <m>S</m> is an inverse of <m>T</m>.

We will now assume that <m>T</m> has an inverse <m>S</m> and show that <m>T</m> must be one-to-one and onto.  
Suppose 
<me>
    T(\mathbf{v}_1)=T(\mathbf{v}_2).
</me>
 then 
<me>
    S(T(\mathbf{v}_1))=S(T(\mathbf{v}_2)),
</me>

but then

<me>
    \mathbf{v}_1=\mathbf{v}_2.
</me>

We conclude that <m>T</m> is one-to-one.

Now suppose that <m>\mathbf{w}</m> is in <m>W</m>.  We need to show that some element of <m>V</m> maps to <m>\mathbf{w}</m>.  Let <m>\mathbf{v}=S(\mathbf{w})</m>.  Then

<me>
    T(\mathbf{v})=T(S(\mathbf{w}))=(T\circ S)(\mathbf{w})=\id_W(\mathbf{w})=\mathbf{w}.
</me>

We conclude that <m>T</m> is onto.
    </p>
</proof>
</theorem>


<p> 
The theorem together with its proof is all very formal. In practice, one can verify whether an inverse exists by verifying onto and one-to-one (often easier than pinpointing an inverse).
Here is a case in point:
</p> 




<example xml:id="ex-subtosubinvert">
    <statement>
        <p>
            Transformation <m>T</m> in <xref ref="ex-subtosub"/>  is invertible.
       </p>
    </statement>
    <answer>
        <p>
            We demonstrated that <m>T</m> is one-to-one and onto.  By <xref ref="th-isomeansinvert"/>, <m>T</m> has an inverse.  

Recall that <m>T</m> was introduced to demonstrate that <xref ref="th-existunique"/> is not always directly applicable.  We now have additional tools. <xref ref="th-isomeansinvert"/> assures us that <m>T</m> has an inverse, but does not help us find it. We will visit this problem again in later sections and find an inverse of <m>T</m>.
       </p>
    </answer>
</example>









<p>
Having an inverse refers to <m>S</m> as <term>an</term> inverse of <m>T</m>, 
implying that there may be more than one such transformation <m>S</m>.  
We will now show that if such a transformation <m>S</m> exists, it is unique.  
This will allow us to refer to it as <em>the</em> inverse of <m>T</m> and to start using <m>T^{-1}</m> to denote the unique inverse of <m>T</m>.
</p> 

<theorem xml:id="th-inverseisunique">

    <statement>
        <p>
            If <m>T</m> is a linear transformation, and <m>S</m> is an inverse of <m>T</m>.  Then <m>S</m> is unique.
        </p>
    </statement>

<proof>
    <p>
Let <m>T:V\rightarrow W</m> be a linear transformation.  If <m>S</m> is an inverse of <m>T</m>, then <m>S</m> satisfies

<me>
    S\circ T=\id_V\quad \text{and}\quad T\circ S=\id_W.
</me>


Suppose there is another transformation, <m>S'</m>, such that 

<me>
    S'\circ T=\id_V\quad \text{and}\quad T\circ S'=\id_W.
</me>

We now show that <m>S=S'</m>.

<me>
    S=S\circ \id_W=S\circ(T\circ S')=(S\circ T)\circ S'=\id_V\circ S'=S'.
</me>

    </p>
</proof>
</theorem>
</subsection>




























<exercises>
<exercise xml:id="prob-lintransP2toM22">
    <statement>
        <p>
            Suppose <m>T:\mathbb{P}^2\rightarrow\mathbb{M}_{2,2}</m> is a linear transformation such that 

<me>
    T(1)=\begin{bmatrix}1\amp 0\\0\amp 1\end{bmatrix},\quad T(x)=\begin{bmatrix}1\amp 1\\0\amp 1\end{bmatrix},\quad T(x^2)=\begin{bmatrix}1\amp 1\\1\amp 1\end{bmatrix}
</me>

Find <me>T(4-x+3x^2)</me>.
        </p>
    </statement>

    <answer>
        <p>
            <me>
    T(4-x+3x^2)=\begin{bmatrix}6\amp 2\\3\amp 6\end{bmatrix}.
</me>
        </p>
    </answer>
</exercise>

<exercise xml:id="prob-tracelintrans1">
    <statement>
        <p>
            Define <m>T:\mathbb{M}_{3,3}\rightarrow \R</m> by <m>T(A)=\mbox{tr}(A)</m>.  (Recall that <m>\mbox{tr}(A)</m> denotes the <term>trace</term> of <m>A</m>, which is the sum of the main diagonal entries of <m>A</m>.)

Find <me>T\left(\begin{bmatrix}1\amp 2\amp 3\\4\amp 5\amp 6\\7\amp 8\amp 9\end{bmatrix}\right).</me>
        </p>
    </statement>
    <answer>
        <p>
            <me>
    T\left(\begin{bmatrix}1\amp 2\amp 3\\4\amp 5\amp 6\\7\amp 8\amp 9\end{bmatrix}\right)=15.
</me>
        </p>
    </answer>
</exercise>

<exercise xml:id="prob-tracelintrans2">
    <statement>
        <p>
            Is <m>T</m> a linear transformation?  If so, prove it.  If not, give a counterexample.
        </p>
    </statement>
</exercise>


<exercise xml:id="prob-lintransr2toM22part1">
    <statement>
        <p>
            Define <m>T:\R^2\rightarrow\mathbb{M}_{2,2}</m> by 
            <me>
            T\left(\begin{bmatrix}a\\b\end{bmatrix}\right)=\begin{bmatrix}a\amp 1\\1\amp b\end{bmatrix}.
           </me>

Find <me>
    T\left(\begin{bmatrix}2\\-1\end{bmatrix}\right).
    </me>
        </p>
    </statement>
    <answer>
        <p>
            <me>
    T\left(\begin{bmatrix}2\\-1\end{bmatrix}\right)=\begin{bmatrix}2\amp 1\\1\amp -1\end{bmatrix}.
</me>
        </p>
    </answer>

</exercise>
<exercise xml:id="prob-lintransr2toM22part2">
    <statement>
        <p>
            Is <m>T</m> a linear transformation?  If so, prove it.  If not, give a counterexample.
        </p>
    </statement>
</exercise>


<exercise xml:id="prob-detlintrans1">
    <statement>
        <p>
            This problem requires the knowledge of how to compute a <m>3\times 3</m> determinant (for a quick reminder, chapter <m>1</m>).
Define <m>T:\mathbb{M}_{3,3}\rightarrow \R</m> by <m>T(A)=\det(A)</m>.  


Find <me>
     T\left(\begin{bmatrix}1\amp 2\amp 3\\4\amp 5\amp 6\\7\amp 8\amp 9\end{bmatrix}\right).
    </me>
        </p>
    </statement>
    <answer>
        <p>
            <me>
    T\left(\begin{bmatrix}1\amp 2\amp 3\\4\amp 5\amp 6\\7\amp 8\amp 9\end{bmatrix}\right)=0.
</me>
        </p>
    </answer>
</exercise>

<exercise xml:id="prob-detlintrans2">
    <statement>
        <p>
            Is <m>T</m> a linear transformation?  If so, prove it.  If not, give a counterexample.
        </p>
    </statement>
</exercise>


<exercise xml:id="prob-lintransderivative1">
    <statement>
        <p>
            Define <m>T:\mathbb{P}^3\rightarrow\mathbb{P}^2</m> by <m>T(p(x))=p'(x)</m> (in other words, <m>T</m> maps a polynomial to its derivative).
Find <me>T(4x^3-2x^2+x+6).</me>
        </p>
    </statement>
    <answer>
        <p>
            <me>
    T(4x^3-2x^2+x+6)=12x^2-4x+1.
</me>
        </p>
    </answer>
</exercise>

<exercise xml:id="prob-lintransderivative2">
    <statement>
        <p>
            Is <m>T</m> a linear transformation?  If so, prove it.  If not, give a counterexample.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-symmMatLinTrans">
    <statement>
        <p>
            Recall that the set <m>V</m> of all symmetric <m>2\times 2</m> matrices is a subspace of <m>\mathbb{M}_{2,2}</m>.  In <xref ref="ex-symmetricmatsubspace"/>, we demonstrated that 
<me>\mathcal{B} = \left\{
\begin{bmatrix}
1 \amp  0 \\
0 \amp  0
\end{bmatrix}, \begin{bmatrix}
0 \amp  0 \\
0 \amp  1
\end{bmatrix}, \begin{bmatrix}
0 \amp  1 \\
1 \amp  0
\end{bmatrix}
\right\}</me>

is a basis for <m>V</m>.  Define <m>T:V\rightarrow \R^3</m> by <m>T(A)=[A]_{\mathcal{B}}</m>.  Find 
<me>
T(I_2) \quad \text{and} \quad T\left(\begin{bmatrix}2\amp -3\\-3\amp 1\end{bmatrix}\right).
</me>
        </p>
    </statement>
    <answer>
        <p>
            <me>
    T(I_2)=\begin{bmatrix}1\\1\\0\end{bmatrix},
</me>


<me>
    T\left(\begin{bmatrix}2\amp -3\\-3\amp 1\end{bmatrix}\right)=\begin{bmatrix}2\\1\\-3\end{bmatrix}.
</me>
        </p>
    </answer>
</exercise>

<exercise xml:id="prob-coordvector">
    <statement>
        <p>
            Let <m>V</m> be a subspace of <m>\R^3</m> with a basis <m>\mathcal{B}=\left\{\begin{bmatrix}2\\1\\-1\end{bmatrix}, \begin{bmatrix}0\\3\\2\end{bmatrix}\right\}</m>.  Find the coordinate vector, <m>[\mathbf{v}]_{\mathcal{B}}</m>, for <m>\mathbf{v}=[4,-1,-4]</m>.
        </p>
    </statement>
            <answer>
            <p>
<me>
    [\mathbf{v}]_{\mathcal{B}}=\begin{bmatrix}2\\-1\end{bmatrix}.
</me>
            </p>
            </answer>
     
</exercise>

<exercise xml:id="prob-switchbasisorder">
    <statement>
        <p>
            If the order of the basis elements in <xref ref="prob-coordvector"/> was switched to form a new basis

<me>
    \mathcal{B}'=\left\{\begin{bmatrix}0\\3\\2\end{bmatrix}, \begin{bmatrix}2\\1\\-1\end{bmatrix} \right\}.
</me>

How would this affect the coordinate vector?
</p>
</statement>
    <answer>
        <p>
<me>
    [\mathbf{v}]_{\mathcal{B}'}=\begin{bmatrix}-1\\2\end{bmatrix}
</me>
        </p>
    </answer>
</exercise>



<exercise xml:id="prob-polylintranscoordvect">
    <statement>
        <p>
            In <xref ref="prob-linindabstractvsp123"/>, you demonstrated that
<me>
\mathcal{B}=\{x^{2}, x + 1, 1 - x - x^{2}\}
</me>
 is a basis for <m>\mathbb{P}^2</m>.  Define <m>T:\mathbb{P}^2\rightarrow \R^3</m> by <m>T(p(x))=[p(x)]_{\mathcal{B}}</m>.  Find 
 <me>
T(0), \quad T(x+1) \quad \text{and} \quad T(x^2-3x+1).
</me>
        </p>
    </statement>
    <answer>
        <p>
            <me>
    T(0)=\begin{bmatrix}0\\0\\0\end{bmatrix},
</me>


<me>
    T(x+1)=\begin{bmatrix}0\\1\\0\end{bmatrix},
</me>


<me>
    T(x^2-3x+1)=\begin{bmatrix}3\\-1\\2\end{bmatrix}.
</me>
        </p>
    </answer>
</exercise>





<exercise xml:id="prob-lintransandbasis4">
    <statement>
        <p>
            Let <m>V</m> and <m>W</m> be vector spaces, and let <m>\mathcal{B}_V=\{\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3, \mathbf{v}_4\}</m> and <m>\mathcal{B}_W=\{\mathbf{w}_1,\mathbf{w}_2, \mathbf{w}_3\}</m> be ordered bases of <m>V</m> and <m>W</m>, respectively.  Suppose <m>T:V\rightarrow W</m> is a linear transformation such that: 
<me>
    T(\mathbf{v}_1)=\mathbf{w}_2,
</me>
 
<me>
    T(\mathbf{v}_2)=2\mathbf{w}_1-3\mathbf{w}_2,
</me>


<me>
    T(\mathbf{v}_3)=\mathbf{w}_2+\mathbf{w}_3,
</me>


<me>
    T(\mathbf{v}_4)=-\mathbf{w}_1.
</me>

If <m>\mathbf{v}=-2\mathbf{v}_1+3\mathbf{v}_2-\mathbf{v}_4</m>, express <m>T(\mathbf{v})</m> as a linear combination of vectors of <m>\mathcal{B}_W</m>. Now,

<me>
    T(\mathbf{v})=7\mathbf{w}_1-11\mathbf{w}_2+0\mathbf{w}_3.
</me>

Find <m>[\mathbf{v}]_{\mathcal{B}_V}</m> and <m>[T(\mathbf{v})]_{\mathcal{B}_{W}}</m>. 
</p>
</statement>

<answer>
<p>
<me>
    [\mathbf{v}]_{\mathcal{B}_V}=\begin{bmatrix}-2\\3\\0\\-1\end{bmatrix},\quad [T(\mathbf{v})]_{\mathcal{B}_{W}}=\begin{bmatrix}7\\-11\\0\end{bmatrix}.
</me>
</p>
</answer>
</exercise>



<exercise xml:id="prob-completeproofoflin">
    <statement>
        <p>
            Complete the proof of <xref ref="th-coordvectmappinglinear"/>.
        </p>
    </statement>
</exercise>



<!--Inverse exercises start here-->


<exercise xml:id="prob-inverses1">
    <statement>
        <p>
            Show that a linear transformation <m>T:\R^2\rightarrow \R^2</m> with standard matrix 
            <me>
            A=\begin{bmatrix}2\amp -4\\-3\amp 6\end{bmatrix}
            </me>
            
            is not one-to-one.
        </p>
    </statement>

   <hint>
    <p>
        Show that multiple vectors map to <m>\mathbf{0}</m>.
    </p>
   </hint>
</exercise>
 
 <exercise xml:id="prob-inverses2">
    <statement>
        <p>
            Show that a linear transformation <m>T:\R^2\rightarrow \R^3</m> with standard matrix 
        <me>
        A=\begin{bmatrix}1\amp 2\\-1\amp 1\\0\amp 1\end{bmatrix}
        </me>
        
        is not onto.
        </p>
    </statement>

    <hint>
    <p>
    Find <m>\mathbf{b}</m> such that <m>A\mathbf{x}=\mathbf{b}</m> has no solutions.
    </p>
    </hint>
</exercise>
 
 <exercise xml:id="prob-inverses3">
    <statement>
        <p>
            Suppose that a linear transformation <m>T:\R^3\rightarrow \R^3</m> has a standard matrix <m>A</m> such that <m>\text{rref}(A)=I</m>.
 
 Prove that <m>T</m> is one-to-one and onto
        </p>
    </statement>

    <hint>
        <p>
    For the one-to-one verification, does <m>A\mathbf{x}=\mathbf{b}</m> have a solution for every <m>\mathbf{b}</m>?  
        </p>
    </hint>

    <hint>
        <p>
    For the onto verification, how many solutions does <m>A\mathbf{x}=\mathbf{b}</m> have?
        </p>
    </hint>

</exercise>
 


 <exercise xml:id="prob-inverses4">
    <statement>
        <p>
            Define a transformation <m>T:\R^2\rightarrow \R^2</m> by 
 
<me>
    T\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}x\\-2x+4y\end{bmatrix}.
</me>

 Show that <m>T</m> is a linear transformation that has an inverse.
        </p>
    </statement>

    <hint>
        <p>
    You will need to demonstrate that <m>T</m> is one-to-one and onto.
        </p>
    </hint>
</exercise>
 
 <exercise xml:id="prob-inverses5">
    <statement>
        <p>
            Let 
        <me>
        V=\text{span}\left(\begin{bmatrix}1\\0\\1\end{bmatrix}, \begin{bmatrix}0\\1\\0\end{bmatrix}\right).
        </me>
        
        Define a linear transformation <m>T:V\rightarrow \R^2</m> by
 
<me>
    T\left(\begin{bmatrix}1\\0\\1\end{bmatrix}\right)=\begin{bmatrix}0\\1\end{bmatrix}\quad \text{and}\quad T\left(\begin{bmatrix}0\\1\\0\end{bmatrix}\right)=\begin{bmatrix}-1\\1\end{bmatrix}
</me>

 Prove that <m>T</m> has an inverse.
        </p>
    </statement>
</exercise>


</exercises>
</section>