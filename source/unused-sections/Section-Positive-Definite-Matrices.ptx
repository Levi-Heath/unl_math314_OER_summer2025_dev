<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="Section-Positive-Definite-Matrices" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Extra Topic: Positive Definite Matrices</title>



  



<p> 
All the eigenvalues of any symmetric
matrix are real (deduced in later sections; this section is about the case in which those eigenvalues
 are positive. These matrices, which arise whenever optimization
(maximum and minimum) problems are encountered, have countless
applications throughout science and engineering. They also arise in
statistics (for example, in factor analysis used in the social sciences)
 and in geometry (quadratic forms, for instance). 
</p> 


<definition xml:id="def-024811">

    <statement>
        <p>
            A square matrix is called <term>positive definite</term> if it is symmetric and all its eigenvalues <m>\lambda</m> are positive. 
            We write <m>\lambda\gt 0</m> when eigenvalues are real and positive.
        </p>
    </statement>
</definition>

<p> 
Because these matrices are symmetric, the Real Spectral (<xref ref="th-PrinAxes"/>) plays a central role in the theory.
</p> 

<theorem xml:id="thm-024815">

    <statement>
        <p>
            If <m>A</m> is positive definite, then it is invertible and <m>\det A \gt  0</m>.
        </p>
    </statement>


<proof>
    <p>
If <m>A</m> is <m>n \times n</m> and the eigenvalues are <m>\lambda_{1}, \lambda_{2}, \dots, \lambda_{n}</m>, then
<me>
\det A = \lambda_{1}\lambda_{2} \cdots \lambda_{n} \gt  0
</me>

by the Real Spectral Theorem.
    </p>
</proof>
</theorem>

<p> 
If <m>\mathbf{x}</m> is a column in <m>\R^n</m> and <m>A</m> is any real <m>n \times n</m> matrix, 
we view the <m>1 \times 1</m> matrix <m>\mathbf{x}^{T}A\mathbf{x}</m> as a real number. With this convention, 
we have the following characterization of positive definite matrices.
</p> 

<theorem xml:id="thm-024830">

    <statement>
        <p>
            A symmetric matrix <m>A</m> is positive definite if and only if <m>\mathbf{x}^{T} A \mathbf{x} \gt  0</m> 
            for every column <m>\mathbf{x} \neq \mathbf{0}</m> in <m>\R^n</m>.
        </p>
    </statement>


<proof>
    <p>
<m>A</m> is symmetric.  Using  the Real Spectral Theorem, let 
<me>
Q^{T}AQ = D = \mbox{diag}(\lambda_{1}, \lambda_{2}, \dots, \lambda_{n})
</me>

where <m>Q^{-1} = Q^{T}</m> and the <m>\lambda_{i}</m> are the eigenvalues of <m>A</m>. 
Given a column <m>\mathbf{x}</m> in <m>\R^n</m>, write 
<me>
\mathbf{y} = Q^{T}\mathbf{x} = \left[ \begin{array}{cccc}
y_{1} \amp  y_{2} \amp  \dots \amp  y_{n}
\end{array}\right]^T.
</me>

Then
<mdn >
<mrow number="no"> \mathbf{x}^TA\mathbf{x} \amp = \mathbf{x}^T(QDQ^T)\mathbf{x} </mrow>
<mrow number="no"> \amp = \mathbf{y}^TD\mathbf{y} </mrow>
<mrow xml:id="symmetricEq"> \amp = \lambda_{1}y_{1}^2 + \lambda_{2}y_{2}^2 + \dots + \lambda_{n}y_{n}^2. </mrow> 

</mdn>
If <m>A</m> is positive definite and <m>\mathbf{x} \neq \mathbf{0}</m>, then <m>\mathbf{x}^{T}A\mathbf{x} \gt  0</m> by <xref ref="symmetricEq"/> because some <m>y_{j} \neq 0</m> and every <m>\lambda_{i} \gt  0</m>.
</p> 

    <p> 
Conversely, if <m>\mathbf{x}^{T}A\mathbf{x} \gt  0</m> whenever <m>\mathbf{x} \neq \mathbf{0}</m>, let <m>\mathbf{x} = Q\mathbf{e}_{j} \neq \mathbf{0}</m> where <m>\mathbf{e}_{j}</m> is column <m>j</m> of <m>I_{n}</m>. Then <m>\mathbf{y} = \mathbf{e}_{j}</m>, so <xref ref="symmetricEq"/> reads <m>\lambda_{j} = \mathbf{x}^{T}A\mathbf{x} \gt  0</m>.
    </p>
</proof>
</theorem>


<p> 
Note that <xref ref="thm-024830"/> shows that the positive definite matrices are 
exactly the symmetric matrices <m>A</m> for which the quadratic form <m>q = \mathbf{x}^{T}A\mathbf{x}</m> takes only positive values.  
</p> 


<example xml:id="exa-024865">
    <statement>
        <p>
            If <m>U</m> is any invertible <m>n \times n</m> matrix, show that <m>A = U^{T}U</m> is positive definite.
       </p>
    </statement>


    <answer>
        <p>
            If <m>\mathbf{x}</m> is in <m>\R^n</m> and <m>\mathbf{x} \neq \mathbf{0}</m>, then
<me>
\mathbf{x}^TA\mathbf{x} =  \mathbf{x}^T(U^TU)\mathbf{x} = (U\mathbf{x})^T(U\mathbf{x})= \norm{ U\mathbf{x} }^2 \gt  0,
</me>

because <m>U\mathbf{x} \neq \mathbf{0}</m> (<m>U</m> is invertible). Hence <xref ref="thm-024830"/> applies.
       </p>
    </answer>
</example>

<p> 
It is remarkable that the converse to <xref ref="exa-024865"/> is also true. In fact every positive definite matrix <m>A</m> 
can be factored as <m>A = U^{T}U</m> where <m>U</m> is an upper triangular matrix with positive elements on the main
diagonal. However, before verifying this, we introduce another concept
that is central to any discussion of positive definite matrices.
</p> 

<p> 
If <m>A</m> is any <m>n \times n</m> matrix, let <m>^{(r)}A</m> denote the <m>r \times r</m> submatrix in the upper left corner of <m>A</m>; 
that is, <m>^{(r)}A</m> is the matrix obtained from <m>A</m> by deleting the last <m>n - r</m> rows and columns. 
</p> 

<p> 
The matrices <m>^{(1)}A, ^{(2)}A, ^{(3)}A, \dots</m>, <m>^{(n)}A = A</m> are called the <term>principal submatrices</term> of <m>A</m>.
</p> 

<p>
To get accustomed to principal matrices, we will analyse a concrete case.
</p> 

<example xml:id="exa-024883">
    <p>
        If <m>A = \left[ \begin{array}{rrr}
10 \amp  5 \amp  2 \\
5 \amp  3 \amp  2 \\
2 \amp  2 \amp  3
\end{array}\right]</m> 

then
<me> ^{(1)}A = \left[ 10 \right], \quad ^{(2)}A = \left[ \begin{array}{rr}
10 \amp  5 \\
5 \amp  3
\end{array}\right] \text{ and } ^{(3)}A = A.
</me>
    </p>
</example>

<lemma xml:id="lem-024890">

    <statement>
        <p>
            If <m>A</m> is positive definite, so is each principal submatrix <m>^{(r)}A</m> for <m>r = 1, 2, \dots, n</m>.
        </p>
    </statement>


<proof>
    <p>
Write
<me>
A = \left[ \begin{array}{rr}
^{(r)}A \amp  P \\
Q \amp  R
\end{array}\right]
</me>


 in block form. If <m>\mathbf{y} \neq \mathbf{0}</m> in <m>\R^r</m>, write 
 <me>
\mathbf{x} = \left[ \begin{array}{r}
 \mathbf{y} \\
 \mathbf{0}
 \end{array}\right]
</me>

in <m>\R^n</m>. Then <m>\mathbf{x} \neq \mathbf{0}</m>, so the fact that <m>A</m> is positive definite gives
<me>
0 \lt  \mathbf{x}^TA\mathbf{x} = \left[ \begin{array}{rr}
\mathbf{y}^T \amp  \mathbf{0}
\end{array}\right] \left[ \begin{array}{rr}
^{(r)}A \amp  P \\
Q \amp  R
\end{array}\right] \left[ \begin{array}{r}
\mathbf{y} \\
\mathbf{0}
\end{array}\right] = \mathbf{y}^T(^{(r)}A)\mathbf{y}.
</me>
This shows that <m>^{(r)}A</m> is positive definite by <xref ref="thm-024830"/>. 
A similar argument shows that, if <m>B</m> is any matrix obtained from a positive definite matrix <m>A</m> by deleting certain rows and 
deleting the <em>same</em> columns, then <m>B</m> is also positive definite.
    </p>
</proof>
</lemma>

<p> 
If <m>A</m> is positive definite, <xref ref="lem-024890"/> and <xref ref="thm-024815"/> show that <m>\mbox{det}(^{(r)}A) \gt  0</m> for every <m>r</m>.
 This proves part of the following theorem which contains the converse to <xref ref="exa-024865"/>,
  and characterizes the positive definite matrices among the symmetric ones.
</p> 




<theorem xml:id="thm-024907">

    <statement>
        <p>
            The following conditions are equivalent for a symmetric <m>n \times n</m> matrix <m>A</m>:

<ol>
<li xml:id="thm-024907a">
  <p> <m>A</m> is positive definite. </p>
</li>

<li xml:id="thm-024907b">
  <p>  <m>\mbox{det}(^{(r)}A) \gt  0</m> for each <m>r = 1, 2, \dots, n</m>. </p>
</li>

<li xml:id="thm-024907c">
  <p>  <m>A = U^{T}U</m> where <m>U</m> is an upper triangular matrix with positive entries on the main diagonal. </p>
</li>
</ol>

Furthermore, the factorization in <xref ref="thm-024907c"/> is unique (called the <term>Cholesky factorization</term> of <m>A</m>).
        </p>
    </statement>



<proof>
    <p>
First, <xref ref="thm-024907c"/> <m>\Rightarrow</m> <xref ref="thm-024907a"/> by <xref ref="exa-024865"/>, and <xref ref="thm-024907a"/> <m>\Rightarrow</m> <xref ref="thm-024907b"/> by <xref ref="lem-024890"/> and <xref ref="thm-024815"/>.
    </p> 

    <p> 
<xref ref="thm-024907b"/> <m>\Rightarrow</m> <xref ref="thm-024907c"/>. Assume <xref ref="thm-024907b"/> and proceed by induction on <m>n</m>. If <m>n = 1</m>, 
then <m>A = \left[ a \right]</m> where <m>a \gt  0</m> by <xref ref="thm-024907b"/>,
so take <m>U = \left[ \sqrt{a} \right]</m>. If <m>n \gt  1</m>, write <m>B =^{(n-1)}A</m>.
 Then <m>B</m> is symmetric and satisfies <xref ref="thm-024907b"/> so, by induction, 
 we have <m>B = U^{T}U</m> as in <xref ref="thm-024907c"/> where <m>U</m> is of size <m>(n - 1) \times (n - 1)</m>. Then, as <m>A</m> is symmetric, 
 it has block form 
<me>
A = \left[ \begin{array}{cc}
B \amp  \mathbf{p} \\
\mathbf{p}^T \amp  b
\end{array}\right]</me>
where <m>\mathbf{p}</m> is a column in <m>\R^{n-1}</m> and <m>b</m> is in <m>\R</m>. 
If we write <m>\mathbf{x} = (U^{T})^{-1}\mathbf{p}</m> and <m>c = b - \mathbf{x}^{T}\mathbf{x}</m>, block multiplication gives
<me>
A = \left[ \begin{array}{cc}
U^TU \amp  \mathbf{p} \\
\mathbf{p}^T \amp  b
\end{array}\right] = \left[ \begin{array}{cc}
U^T \amp  0 \\
\mathbf{x}^T \amp  1
\end{array}\right] \left[ \begin{array}{cc}
U \amp  \mathbf{x} \\
0 \amp  c
\end{array}\right].
</me>

Taking determinants gives <m>\det A = \mbox{det}(U^{T}) \mbox{det} U \cdot c = c(\mbox{det}U)^{2}</m>. Hence <m>c \gt  0</m> because <m>\det A \gt  0</m> by <xref ref="thm-024907b"/>, so the above factorization can be written
<me>
A = \left[ \begin{array}{cc}
U^T \amp  0 \\
\mathbf{x}^T \amp  \sqrt{c}
\end{array}\right] \left[ \begin{array}{cc}
U \amp  \mathbf{x} \\
0 \amp  \sqrt{c}
\end{array}\right].
</me>
Since <m>U</m> has positive diagonal entries, this proves <xref ref="thm-024907c"/>.
</p> 


    <p> 
To prove uniqueness, suppose that <m>A = U^TU = U_{1}^TU_{1}</m> are two Cholesky factorizations. 
Now write
<me>
D = UU_{1}^{-1} = (U^T)^{-1}U_{1}^T.
</me> 

Then <m>D</m> is upper triangular, because <m>D = UU_{1}^{-1}</m>, and lower triangular, 
because
<me>
D = (U^T)^{-1}U_{1}^T,
</me>

and so it is a diagonal matrix. Thus <m>U = DU_{1}</m> and <m>U_{1} = DU</m>, so it suffices to show that <m>D = I</m>. 
But eliminating <m>U_{1}</m> gives <m>U = D^{2}U</m>, so <m>D^{2} = I</m> because <m>U</m> is invertible. Since the diagonal entries of <m>D</m> are positive 
(this is true of <m>U</m> and <m>U_{1}</m>), it follows that <m>D = I</m>.
    </p>
</proof>
</theorem>

<p> 
The remarkable thing is that the matrix <m>U</m> in the Cholesky factorization is easy to obtain from <m>A</m> using row operations. The key is that Step 1 of the following algorithm is <em>possible</em> for any positive definite matrix <m>A</m>. A proof of the algorithm is given following Example~<xref ref="exa-024959"/>.
</p> 





<theorem xml:id="thm-024947">
    <title>Algorithm for the Cholesky Factorization</title>
    <statement>
        <p>
            If <m>A</m> is a positive definite matrix, the Cholesky factorization <m>A = U^{T}U</m> can be obtained as follows:


<ul>
<li>
      <p> [Step 1.] Carry <m>A</m> to an upper triangular matrix <m>U_{1}</m> with positive diagonal entries 
      using row operations each of which adds a multiple of a row to a lower row. </p>
</li>

<li>
      <p> [Step 2.] Obtain <m>U</m> from <m>U_{1}</m> by dividing each row of <m>U_{1}</m> by the square root of the diagonal entry in that row. </p>
</li>

</ul>
        </p>
    </statement>



    <proof>
        <p>
    If <m>A</m> is positive definite, let <m>A = U^{T}U</m> be the Cholesky factorization, 
    and let <m>D = \mbox{diag}(d_{1}, \dots, d_{n})</m> be the common diagonal of <m>U</m> and <m>U^{T}</m>. 
    Then <m>U^{T}D^{-1}</m> is lower triangular with ones on the diagonal (call such matrices LT-1). 
    Hence <m>L = (U^{T}D^{-1})^{-1}</m> is also LT-1, and so <m>I_{n} \to L</m> by a sequence of row operations each of which adds a multiple of a row
    to a lower row (modify columns right to left).  But then <m>A \to LA</m> by the same sequence of row operations.  Since
    <me>
    LA = [D(U^{T})^{-1}][U^{T}U] = DU-
    </me>
    is upper triangular with positive entries on the diagonal, this shows that Step 1 of the algorithm is possible.
    </p> 

    <p> 
    Turning to Step 2, let <m>A \to U_{1}</m> as in Step 1 so that <m>U_{1} = L_{1}A</m> where <m>L_{1}</m> is
    LT-1. Since A is symmetric, we get
    <men xml:id="symmetricEq2">
    L_{1}U_{1}^T = L_{1}(L_{1}A)^T = L_{1}A^TL_{1}^T = L_{1}AL_{1}^T = U_{1}L_{1}^T
    </men>
    Let <m>D_{1} = \mbox{diag}(e_{1}, \dots, e_{n})</m> denote the diagonal of <m>U_{1}</m>. Then <xref ref="symmetricEq2"/> gives
    <me>
    L_{1}(U_{1}^TD_{1}^{-1}) = U_{1}L_{1}^TD_{1}^{-1}.
    </me>
    This is both upper triangular (right side) and LT-1 (left side), and so must equal <m>I_{n}</m>. 
    In particular, <m>U_{1}^TD_{1}^{-1} = L_{1}^{-1}</m>. Now let
    <me>
    D_{2} = \mbox{diag}(\sqrt{e_{1}}, \dots, \sqrt{e_{n}}),
    </me>
    so that <m>D_{2}^2 = D_{1}</m>. 
    If we write <m>U = D_{2}^{-1}U_{1}</m> we have
    <md>
    <mrow> U^TU \amp = (U_{1}^TD_{2}^{-1})(D_{2}^{-1}U_{1}) </mrow>
    <mrow> \amp = U_{1}^T(D_{2}^2)^{-1}U_{1} </mrow> 
    <mrow> \amp = (U_{1}^TD_{1}^{-1})U_{1} </mrow> 
    <mrow> \amp = (L_{1}^{-1})U_{1} = A. </mrow>
    </md>
    This proves Step 2 because <m>U = D_{2}^{-1}U_{1}</m> is formed by dividing each row of <m>U_{1}</m> by the square root of its diagonal entry (verify).
        </p>
    </proof>
</theorem>

<example xml:id="exa-024959">
    <statement>
        <p>
            Find the Cholesky factorization of 
<me>A = \left[ \begin{array}{rrr}
10 \amp  5 \amp  2 \\
5 \amp  3 \amp  2 \\
2 \amp  2 \amp  3
\end{array}\right].
</me>
       </p>
    </statement>
    <answer>
        <p>
            The matrix <m>A</m> is positive definite by <xref ref="thm-024907"/> because <m>\mbox{det}{^{(1)}A} = 10 \gt  0</m>, <m>\mbox{det}{^{(2)}A} = 5 \gt  0</m>, 
            and <m>\mbox{det}{^{(3)}A} = \mbox{det} A = 3 \gt  0</m>. Hence Step 1 of the algorithm is carried out as follows:
<me>
A = \left[ \begin{array}{rrr}
10 \amp  5 \amp  2 \\
5 \amp  3 \amp  2 \\
2 \amp  2 \amp  3
\end{array}\right] \rightarrow \left[ \begin{array}{rrc}
10 \amp  5 \amp  2 \\
0 \amp  \frac{1}{2} \amp  1 \\
0 \amp  1 \amp  \frac{13}{5}
\end{array}\right] \rightarrow \left[ \begin{array}{rrr}
10 \amp  5 \amp  2 \\
0 \amp  \frac{1}{2} \amp  1 \\
0 \amp  0 \amp  \frac{3}{5}
\end{array}\right] = U_{1}.
</me>

Now carry out Step 2 on <m>U_{1}</m> to obtain 
<me>U = \left[ \def\arraystretch{1.5}\begin{array}{ccc}
\sqrt{10} \amp  \frac{5}{\sqrt{10}} \amp  \frac{2}{\sqrt{10}} \\
0 \amp  \frac{1}{\sqrt{2}} \amp  \sqrt{2} \\
0 \amp  0 \amp  \frac{\sqrt{3}}{\sqrt{5}}
\end{array}\right].</me>

The reader can verify that <m>U^{T}U = A</m>.
       </p>
    </answer>
</example>




















<exercises>
<exercise xml:id="prob-pos-def-1">
    <statement>
        <p>
            Find the Cholesky decomposition of each of the following matrices.

<ol>
<li>
      <p>
<me>\left[ \begin{array}{rr}
4 \amp  3 \\
3 \amp  5 
\end{array}\right].
</me>
</p>
</li>


<li>
      <p> 
<me>\left[ \begin{array}{rr}
2 \amp  -1 \\
-1 \amp  1 
\end{array}\right].
</me>
</p>
</li>



<li>
    <p> <m>\left[ \begin{array}{rrr}
12 \amp  4 \amp  3 \\
4 \amp  2 \amp  -1 \\
3 \amp  -1 \amp  7 
\end{array}\right]</m>
</p>
</li>


<li>
    <p> <m>\left[ \begin{array}{rrr}
20 \amp  4 \amp  5 \\
4 \amp  2 \amp  3 \\
5 \amp  3 \amp  5
\end{array}\right]</m>
</p>
</li>
</ol>
</p>
</statement>


<answer>
    <p>
For (b):
<me>U = \frac{\sqrt{2}}{2} \left[ \begin{array}{rr}
    2 \amp  -1 \\
    0 \amp  1
    \end{array}\right]</me>
    
and for (d):
<me>
U = \frac{1}{30} \left[ \begin{array}{ccc}
    60\sqrt{5} \amp  12\sqrt{5} \amp  15\sqrt{5} \\
    0 \amp  6\sqrt{30} \amp  10\sqrt{30} \\
    0 \amp  0 \amp  5\sqrt{15}
    \end{array}\right].
</me>
    </p>
</answer>
</exercise>




<exercise xml:id="prob-pos-def-2">
    <statement>
        <p>
            <ol>
<li>
      <p> If <m>A</m> is positive definite, show that <m>A^{k}</m> is positive definite for all <m>k \geq 1</m>. </p>
</li>

<li>
      <p> Prove the converse to (a) when <m>k</m> is odd. </p>
</li>



<li>
      <p> Find a symmetric matrix <m>A</m> such that <m>A^{2}</m> is positive definite but <m>A</m> is not. </p>
</li>

</ol>
        </p>
    </statement>

<hint> 
<p> 
For (b): If <m>\lambda^{k} \gt  0</m>, <m>k</m> odd, then <m>\lambda \gt  0</m>.
</p> 
</hint>>
</exercise>




<exercise xml:id="prob-pos-def-3">
    <statement>
        <p>
            Let 
<me>
A = \left[ \begin{array}{rr}
1 \amp  a \\
a \amp  b
\end{array}\right].
</me>

If <m>a^{2} \lt  b</m>, show that <m>A</m> is positive definite and find the Cholesky factorization.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-pos-def-4">
    <statement>
        <p>
            If <m>A</m> and <m>B</m> are positive definite and <m>r \gt  0</m>, show that <m>A + B</m> and <m>rA</m> are both positive definite.


        </p>
    </statement>

<answer>
    <p>
If <m>\mathbf{x} \neq \mathbf{0}</m>, then <m>\mathbf{x}^{T}A\mathbf{x} \gt  0</m> and <m>\mathbf{x}^{T}B\mathbf{x} \gt  0</m>. 
Hence 
<me>
\mathbf{x}^{T}(A + B)\mathbf{x} = \mathbf{x}^{T}A\mathbf{x} + \mathbf{x}^{T}B\mathbf{x} \gt  0
</me> 

and <m>\mathbf{x}^{T}(rA)\mathbf{x} = r(\mathbf{x}^{T}A\mathbf{x}) \gt  0</m>, as <m>r \gt  0</m>.
    </p>
</answer>
</exercise>

<exercise xml:id="prob-pos-def-5">
    <statement>
        <p>
            If <m>A</m> and <m>B</m> are positive definite, show that 
<me>\left[ \begin{array}{rr}
A \amp  0 \\
0 \amp  B
\end{array}\right]
</me>
is positive definite.
        </p>
    </statement>
</exercise>



<exercise xml:id="prob-pos-def-6">
    <statement>
        <p>
            If <m>A</m> is an <m>n \times n</m> positive definite matrix and <m>U</m> is an <m>n \times m</m> matrix of rank <m>m</m>, show that <m>U^{T}AU</m> is positive definite.
        </p>
    </statement>

<answer>
    <p>
        Let <m>\mathbf{x} \neq \mathbf{0}</m> in <m>\R^n</m>.
        Then <m>\mathbf{x}^{T}(U^{T}AU)\mathbf{x} = (U\mathbf{x})^{T}A(U\mathbf{x}) \gt  0</m> provided <m>U\mathbf{x} \neq 0</m>. 
        But if
        
        <me>
        U = \left[ \begin{array}{cccc}
            \mathbf{c}_{1} \amp  \mathbf{c}_{2} \amp  \dots \amp   \mathbf{c}_{n}
            \end{array}\right] \text{ and } \mathbf{x} = (x_{1}, x_{2}, \dots, x_{n}),
        </me> then 
        <me>
        U\mathbf{x} = x_{1}\mathbf{c}_{1} + x_{2}\mathbf{c}_{2} + \dots  + x_{n}\mathbf{c}_{n} \neq \mathbf{0},
        </me>
        because <m>\mathbf{x} \neq \mathbf{0}</m> and the <m>\mathbf{c}_{i}</m> are independent.
    </p>
</answer>
</exercise>

<exercise xml:id="prob-pos-def-7">
    <statement>
        <p>
            If <m>A</m> is positive definite, show that each diagonal entry is positive.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-pos-def-8">
    <statement>
        <p>
            Let <m>A_{0}</m> be formed from <m>A</m> by deleting rows 2 and 4 and deleting columns 2 and 4. If <m>A</m> is positive definite, 
            show that <m>A_{0}</m> is positive definite.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-pos-def-9">
    <statement>
        <p>
            If <m>A</m> is positive definite, show that \\ <m>A = CC^{T}</m> where <m>C</m> has orthogonal columns.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-pos-def-10">
    <statement>
        <p>
            If <m>A</m> is positive definite, show that <m>A = C^{2}</m> where <m>C</m> is positive definite.

Click the arrow for answer.


        </p>
    </statement>

<answer>
    <p>
        Let <m>P^{T}AP = D = \mbox{diag}(\lambda_{1}, \dots, \lambda_{n})</m> where <m>P^{T} = P</m>. 
        Since <m>A</m> is positive definite, each eigenvalue <m>\lambda_{i} \gt  0</m>. If <m>B = \mbox{diag}(\sqrt{\lambda_{1}}, \dots, \sqrt{\lambda_{n}})</m> 
        then <m>B^{2} = D</m>, so <m>A = PB^{2}P^{T} = (PBP^{T})^{2}</m>. Take <m>C = PBP^{T}</m>. 
        Since <m>C</m> has eigenvalues <m>\sqrt{\lambda_{i}} \gt  0</m>, it is positive definite.
    </p>
</answer>
</exercise>

<exercise xml:id="prob-pos-def-11">
    <statement>
        <p>
            Let <m>A</m> be a positive definite matrix. If <m>a</m> is a real number, show that <m>aA</m> is positive definite if and only if <m>a \gt  0</m>.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-pos-def-12">
    <statement>
        <p>
            <ol>
<li>
      <p> Suppose an invertible matrix <m>A</m> can be factored in <m>\mathbb{M}_{nn}</m> as <m>A = LDU</m> where <m>L</m> is lower triangular with <m>1</m>s on the diagonal, <m>U</m> is upper triangular with <m>1</m>s on the diagonal, and <m>D</m> is diagonal with positive diagonal entries. Show that the factorization is unique: If <m>A = L_{1}D_{1}U_{1}</m> is another such factorization, show that <m>L_{1} = L</m>, <m>D_{1} = D</m>, and <m>U_{1} = U</m>. </p>
</li>

<li>
      <p> Show that a matrix <m>A</m> is positive definite if and only if <m>A</m> is symmetric and admits a factorization <m>A = LDU</m> as in (a). </p>
</li>
</ol>

        </p>
    </statement>

<answer>
    <p>

        If <m>A</m> is positive definite, use <xref ref="thm-024815"/> to write <m>A = U^{T}U</m> 
        where <m>U</m> is upper triangular with positive diagonal <m>D</m>.  Then 
        <me>
        A = (D^{-1}U)^{T}D^{2}(D^{-1}U)
        </me>
        so <m>A = L_{1}D_{1}U_{1}</m> is such a factorization if <m>U_{1} = D^{-1}U</m>, <m>D_{1} = D^{2}</m>, 
        and <m>L_{1} = U^T_1</m>. 
    </p> 
    
    <p> 
    Conversely, let <m>A^{T} = A = LDU</m> be such a factorization. Then 
    <me>
    U^{T}D^{T}L^{T} = A^{T} = A = LDU,
    </me>  

        so <m>L = U^{T}</m> by \textbf{(a)}. Hence <m>A = LDL^{T} = V^{T}V</m> where <m>V = LD_{0}</m> and <m>D_{0}</m> is diagonal with <m>D^2_0 = D</m> 
        (the matrix <m>D_{0}</m> exists because <m>D</m> has positive diagonal entries). Hence <m>A</m> is symmetric,
         and it is positive definite by <xref ref="exa-024865"/>.
    </p>
</answer>
</exercise>


<exercise xml:id="prob-pos-def-13">
    <statement>
        <p>
            Let <m>A</m> be positive definite and write <m>d_{r} = \det{\left(^{(r)}A\right)}</m> for each <m>r = 1, 2, \dots, n</m>.
            If <m>U</m> is the upper triangular matrix obtained in step 1 of the algorithm,
             show that the diagonal elements <m>u_{11}, u_{22}, \dots, u_{nn}</m> of <m>U</m> are given by <m>u_{11} = d_{1}</m>, <m>u_{jj} = d_{j} / d_{j-1}</m> if <m>j \gt  1</m>. 
        </p>
    </statement>

<answer>
    <p>
        If <m>LA = U</m> where <m>L</m> is lower triangular with <m>1</m>s on the diagonal, 
        use block multiplication to show that
        <me>
        \det{\left(^{(r)}A\right)} = \det{\left(^{(r)}U\right)}
        </me>
        for each <m>r</m>.
    </p>
</answer>
</exercise>


</exercises>
</section>